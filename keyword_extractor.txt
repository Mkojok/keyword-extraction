import re
from collections import Counter

def tokenize(text): 
    return re.findall(r'\w+', text.lower())

tokenized_words = Counter(tokenize(open('train.txt').read()))

def probability(word, N=sum(tokenized_words.values())): 
    return tokenized_words[word] / N

def known(words): 
    return set(w for w in words if w in tokenized_words)

def edit_dist_1(word):
    letters    = 'abcdefghijklmnopqrstuvwxyz'
    #Split the word in two halves in all possible ways
    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]
    #Edit distance for deletion
    deletes    = [left + right[1:]               for left, right in splits if right]
    #Edit distance based on transposition
    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right)>1]
    #Edit distance based on replacing
    replaces   = [left + c + right[1:]           for left, right in splits if right for c in letters]
    #Edit Distance based on insertion
    inserts    = [left + c + right               for left, right in splits for c in letters]
    return set(deletes + transposes + replaces + inserts)

def edit_dist_2(word): 
    #Find words with edit distance : 2
    return (e2 for e1 in edit_dist_1(word) for e2 in edit_dist_1(e1))

def find(text,keyword):
    #Find Keywords in the short text
    words=tokenize(text)
    if(keyword in words):
        return True

#Search for the keyword
keyword='somthing'
import pandas as pd
#Test Data Loaded
test=pd.read_csv('output.csv')
#Short Text
short_text=test.loc[:,'Reviews']
found_key_text=[]
if(keyword in tokenized_words):
    for i in short_text:
        return_cond=find(i,keyword)
        if(return_cond==True):
            found_key_text.append((keyword,i))

else:
    keywords_1=known(edit_dist_1(keyword))
    for keys in keywords_1:
        for i in short_text:
            return_cond=find(i,keys)
            if(return_cond==True):
                found_key_text.append((keys,i))
    
    keywords_2=known(edit_dist_2(keyword))
    for keys in keywords_2:
        for i in short_text:
            return_cond=find(i,keys)
            if(return_cond==True):
                found_key_text.append((keys,i))

from collections import OrderedDict
headers=["Keyword","Text"]
#Print Keywords and Text
json_extract=[]
for keyword,text in found_key_text:
    print('--------------------------------------------------------')
    print("Keyword : "+keyword+'\n',"Text : "+text)
    tmpdict=OrderedDict.fromkeys(headers)
    tmpdict["Keyword"]=keyword
    tmpdict["Text"]=text
    json_extract.append(tmpdict)

#Store Results in Json with Keyword as search keys and Text as short text values
import json
with open('result.json', 'w') as fp:
    json.dump(json_extract, fp)